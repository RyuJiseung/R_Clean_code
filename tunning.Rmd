
---
title: 'Modeling - KKBox EDA'
date: '`r Sys.Date()`'
output:
  html_document:
    number_sections: true
    fig_caption: true
    toc: true
    fig_width: 7
    fig_height: 4.5
    theme: cosmo
    highlight: tango
    code_folding: hide
---

## Load libraries and helper functions
```{r, message = FALSE}
# general visualisation
library(needs)
needs(dplyr,ggplot2,reshape2,scales,grid,gridExtra,RColorBrewer,corrplot,
      readr,MLmetrics,randomForest,gbm,h2o,
        data.table,tibble,tidyr,stringr,forcats,lubridate,ggridges,caret,Matrix,
      xgboost,caret,dplyr)
```



## Load data
```{r warning=FALSE, results=FALSE}
train_df <- read_csv('~/Desktop/r스터디(목)/6_kaggle_competition/data/train_df_jiseung.csv')
test_df <- read_csv('~/Desktop/r스터디(목)/6_kaggle_competition/data/test_df.csv')

train_df <- train_df %>% subset(!msno %in% test_df$msno)

train_df$is_churn <- as.factor(train_df$is_churn)
test_df$is_churn <- as.factor(test_df$is_churn)
```


## 셋 구분
```{r}
seed <- 1
set.seed(seed)
idx <- sample(1:nrow(train_df), size = nrow(train_df)*0.7)
# 훈련 데이터
train <- as.data.frame(train_df[idx,])
train$is_churn<-as.factor(train$is_churn)
# 테스트 데이터
val <- as.data.frame(train_df[!1:nrow(train_df) %in% idx,])
val$is_churn<-as.factor(val$is_churn)
```


## H2O 클러스터
```{r}
h2o.init(nthreads = -1, max_mem_size = '8G')
train$is_churn<-as.factor(train$is_churn)
train_Hex<-as.h2o(train)

val$is_churn<-as.factor(val$is_churn)
val_Hex<-as.h2o(val)

test_df$is_churn <- c(rep(1,(nrow(test_df)+1)/2),rep(0,(nrow(test_df)-1)/2))
test_df$is_churn <- as.factor(test_df$is_churn)
test_Hex<-as.h2o(test_df)
```

## Modeling
```{r}
predictors <- setdiff(names(train),
                       c("is_churn", "msno"))
response <- "is_churn"
```

## gbm
```{r}
gbm <- h2o.gbm(x = predictors,
                      y = response,
                      training_frame = train_Hex,
                      validation_frame = val_Hex,
                      # nfolds=3,
                      ntree=200,
                      seed=950902)

pred_gbm <- h2o.predict(gbm,test_Hex)

test_df$gbm <- as.vector(pred_gbm$p1)

temp <- test_df %>% select(msno,gbm)
colnames(temp) <- c("msno","is_churn")
write.csv(temp,"gbm_jiseung.csv",row.names=FALSE)

gbm@model$variable_importances
```

## xgboost
```{r}
xgboost <- h2o.xgboost(x = predictors,
                      y = response,
                      training_frame = train_Hex,
                      validation_frame = val_Hex,
                      ntree=200,
                      # nfolds=3,
                      seed=950902)

pred_xgboost <- h2o.predict(xgboost,test_Hex)


test_df$xgboost <- as.vector(pred_xgboost$C3)

temp <- test_df %>% select(msno,xgboost)
colnames(temp) <- c("msno","is_churn")
write.csv(temp,"xgboost_jiseung.csv",row.names=FALSE)


xgboost@model$variable_importances

sum(test_df$msno %in% train_df$msno)
table(train_df$is_churn)
```

## rf
```{r}
rf <- h2o.randomForest(x = predictors,
                      y = response,
                      training_frame = train_Hex,
                      validation_frame = val_Hex,
                      ntree=200,
                      # nfolds=3,
                      seed=950902)


pred_rf <- h2o.predict(rf,test_Hex)

test_df$rf <- as.vector(pred_rf$p1)

temp <- test_df %>% select(msno,rf)
colnames(temp) <- c("msno","is_churn")
write.csv(temp,"rf_jiseung.csv",row.names=FALSE)

rf@model$variable_importances
```

## glm
```{r}
glm_x <- h2o.glm(x = predictors
                  ,y = response
                 # ,nfolds=3
                  ,training_frame = train_Hex,
                      validation_frame = val_Hex,
                 family = "binomial",
                 # alpha=0.01,
                 # lambda=1.0E-8,
                      seed=950902
                 # ,
                      # link = "logit"
                 )

pred_glm_x <- h2o.predict(glm_x,test_Hex)

test_df$glm <- as.vector(pred_glm_x$p1)

temp <- test_df %>% select(msno,glm)
colnames(temp) <- c("msno","is_churn")
write.csv(temp,"glm_jiseung.csv",row.names=FALSE)

```

## deeplearning
```{r}
deeplearning <-  h2o.deeplearning(
    training_frame = train_Hex, 
    validation_frame = val_Hex,
    seed = 950902,# validation dataset: used for scoring and early stopping
    # nfolds=3,
   x = predictors,
    y = response
   # ,
#     activation = "Rectifier", # default (a.k.a Relu)
#     hidden = c(200, 200),    # default = 2 hidden layers with 200 neurons each
#     epochs = 1,
#    l1=1.0E-4,
#    input_dropout_ratio= 0.05,
# # How many times the dataset should be iterated
#     variable_importances = TRUE,
#     stopping_metric = "misclassification",
#     stopping_tolerance = 1e-2, # stop when logloss does not improve by >=1% for 2 scoring events
#     stopping_rounds = 2,
#     score_validation_samples = 10000# allows obtaining the variable importance, not enabled by default
)

pred_deeplearning <- h2o.predict(deeplearning,test_Hex)

test_df$deeplearning <- as.vector(pred_deeplearning$p1)

temp <- test_df %>% select(msno,deeplearning)
colnames(temp) <- c("msno","is_churn")
write.csv(temp,"deeplearning_jiseung.csv",row.names=FALSE)




deeplearning@model$variable_importances
```



# best deeplearning
```{r}
# # train samples of the training data for speed 
# sampled_train <- train_Hex
# 
# # specify the list of paramters 
# hyper_params <- list(
#     hidden = list( c(100,100), c(150,150), c(200,200)),
#     input_dropout_ratio = c(0, 0.05),
#     l1 = c(1e-4, 1e-3)
# )
# 
# # performs the grid search
# grid_id <- "dl_grid"
# model_dl_grid <- h2o.grid(
#     algorithm = "deeplearning", # name of the algorithm 
#     grid_id = grid_id, 
#     training_frame = sampled_train,
#     validation_frame = val_Hex, 
#     seed = 950902,# validation dataset: used for scoring and early stopping
#    x = predictors,
#     y = response,  
#     epochs = 1,
#     stopping_metric = "misclassification",
#     stopping_tolerance = 1e-2, # stop when logloss does not improve by >=1% for 2 scoring events
#     stopping_rounds = 2,
#     score_validation_samples = 10000, # downsample validation set for faster scoring
#     hyper_params = hyper_params
# )
# 
# # find the best model and evaluate its performance
# stopping_metric <- 'accuracy'
# sorted_models <- h2o.getGrid(
#     grid_id = grid_id, 
#     sort_by = stopping_metric,
#     decreasing = TRUE
# )
# 
# best_model <- h2o.getModel(sorted_models@model_ids[[2]])
# pred3 <- h2o.predict(best_model, test_Hex)
# 
# 
# test_df$deeplearning_tun_1 <- as.vector(pred3$p1)
# 
# temp <- test_df %>% select(msno,deeplearning_tun_1)
# colnames(temp) <- c("msno","is_churn")
# write.csv(temp,"deeplearning_grid_2.csv",row.names=FALSE)
# 
# 
# best_model <- h2o.getModel(sorted_models@model_ids[[3]])
# pred3 <- h2o.predict(best_model, test_Hex)
# 
# 
# test_df$deeplearning_tun_1 <- as.vector(pred3$p1)
# 
# temp <- test_df %>% select(msno,deeplearning_tun_1)
# colnames(temp) <- c("msno","is_churn")
# write.csv(temp,"deeplearning_grid_1.csv",row.names=FALSE)
```



```{r}
# GLM Hyperparamters
# alpha_opt <- c(0.01,0.1,0.3,0.5,0.7,0.9)
# lambda_opt <- c(1e-4,1e-5,1e-6,1e-7,1e-8)
# hyper_params_glm <- list(alpha = alpha_opt,
#                      lambda = lambda_opt)
# 
# search_criteria_glm <- list(
#   strategy = "RandomDiscrete",
#                         max_models = 100,
#                         seed = 950902)
# 
# grid_glm <- h2o.grid(algorithm = "glm",
#                      x = predictors,
#                     y = response,
#                     family="binomial",
#                     training_frame = train_Hex,
#                     validation_frame = val_Hex,
#                      seed = 950902,
#                      hyper_params = hyper_params_glm,
#                      search_criteria = search_criteria_glm)
# summary(grid_glm)

```



```{r}
# # randomForest Hyperparamters
# # learn_rate_opt <- c(0.01, 0.03)
# max_depth_opt <- c(3, 4, 5, 6, 9)
# sample_rate_opt <- c(0.7, 0.8, 0.9, 1.0)
# col_sample_rate_opt <- c(0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8)
# hyper_params_randomForest <- list(
#   # learn_rate = learn_rate_opt,
#                      max_depth = max_depth_opt,
#                      sample_rate = sample_rate_opt,
#                      col_sample_rate = col_sample_rate_opt)
# 
# search_criteria_randomForest <- list(strategy = "RandomDiscrete",
#                         max_models = 20,
#                         seed = 950902)
# 
# gbm_grid_randomForest <- h2o.grid(algorithm = "randomForest",
#                      grid_id = "rf_grid_binomial",
#                      x = predictors,
#                     y = response,
#                     training_frame = train_Hex,
#                       validation_frame = val_Hex,
#                      ntrees = 200,
#                      seed = 950902,
#                      hyper_params = hyper_params_randomForest,
#                      search_criteria = search_criteria_randomForest)

```


```{r}
temp <- test_df %>% select(msno, xgboost, gbm, rf, glm, deeplearning)
temp$is_churn <- temp %>% with((xgboost + gbm + rf + glm + deeplearning)/5)
temp <- temp %>% select(msno, is_churn)
write.csv(temp,"total_jiseung.csv",row.names=FALSE)
```