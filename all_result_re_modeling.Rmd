
---
title: 'Modeling - KKBox EDA'
date: '`r Sys.Date()`'
output:
  html_document:
    number_sections: true
    fig_caption: true
    toc: true
    fig_width: 7
    fig_height: 4.5
    theme: cosmo
    highlight: tango
    code_folding: hide
---

## Load libraries and helper functions
```{r, message = FALSE}
# general visualisation
library(needs)
needs(dplyr,ggplot2,reshape2,scales,grid,gridExtra,RColorBrewer,corrplot,
      readr,MLmetrics,randomForest,gbm,h2o,
        data.table,tibble,tidyr,stringr,forcats,lubridate,ggridges,caret,Matrix,
      xgboost,caret,dplyr)
```



## Load data
```{r warning=FALSE, results=FALSE}
train_df <- read_csv('~/Desktop/r스터디(목)/6_kaggle_competition/data/train_df_train_real.csv')
test_df <- read_csv('~/Desktop/r스터디(목)/6_kaggle_competition/data/test_df.csv')

train_df$is_churn <- as.factor(train_df$is_churn) 
test_df$is_churn <- as.factor(test_df$is_churn)

train_df <- train_df %>% select(-(24:39))
test_df <- test_df %>% select(-(24:39))
```


## 셋 구분
```{r}
seed <- 1
set.seed(seed)
idx <- sample(1:nrow(train_df), size = nrow(train_df)*0.5)
# 훈련1 데이터
train1 <- as.data.frame(train_df[idx,])
train1$is_churn<-as.factor(train1$is_churn)

temp <- as.data.frame(train_df[!1:nrow(train_df) %in% idx,])
idx <- sample(1:nrow(temp), size = nrow(temp)*0.5)

# 훈련2 데이터
train2 <- as.data.frame(temp[idx,])
train2$is_churn<-as.factor(train2$is_churn)

# 검증 데이터
val <- as.data.frame(temp[!1:nrow(temp) %in% idx,])
val$is_churn<-as.factor(val$is_churn)
```


## H2O 클러스터
```{r}
h2o.init(nthreads = -1, max_mem_size = '8G')
train1$is_churn<-as.factor(train1$is_churn)
train1_Hex<-as.h2o(train1)

train2$is_churn<-as.factor(train2$is_churn)
train2_Hex<-as.h2o(train2)

val$is_churn<-as.factor(val$is_churn)
val_Hex<-as.h2o(val)

test_df$is_churn <- c(rep(1,(nrow(test_df)+1)/2),rep(0,(nrow(test_df)-1)/2))
test_df$is_churn <- as.factor(test_df$is_churn)
test_Hex<-as.h2o(test_df)
```

## Modeling
```{r}
predictors <- setdiff(names(train),
                       c("is_churn", "msno"))
response <- "is_churn"
```

## gbm
```{r}
gbm <- h2o.gbm(x = predictors,
                      y = response,
                      training_frame = train1_Hex,
                      validation_frame = train2_Hex,
                      ntree=200,
                      seed=950902)

gbm

pred_gbm_train2 <- h2o.predict(gbm,train2_Hex)
pred_gbm_val <- h2o.predict(gbm,val_Hex)
pred_gbm_test <- h2o.predict(gbm,test_Hex)

train2$gbm <- as.vector(pred_gbm_train2$p1)
val$gbm <- as.vector(pred_gbm_val$p1)
test_df$gbm <- as.vector(pred_gbm_test$p1)

# temp <- test_df %>% select(msno,gbm)
# colnames(temp) <- c("msno","is_churn")
# write.csv(temp,"gbm_only_trans.csv",row.names=FALSE)
# 갭을 변수로 넣자!
gbm@model$variable_importances
```

## xgboost
```{r}
xgboost <- h2o.xgboost(x = predictors,
                      y = response,
                      training_frame = train1_Hex,
                      validation_frame = train2_Hex,
                      ntree=200,
                      seed=950902)

xgboost

pred_xgboost_train2 <- h2o.predict(xgboost,train2_Hex)
pred_xgboost_val <- h2o.predict(xgboost,val_Hex)
pred_xgboost_test <- h2o.predict(xgboost,test_Hex)

train2$xgboost <- as.vector(pred_xgboost_train2$C3)
val$xgboost <- as.vector(pred_xgboost_val$C3)
test_df$xgboost <- as.vector(pred_xgboost_test$C3)

# temp <- test_df %>% select(msno,xgboost)
# colnames(temp) <- c("msno","is_churn")
# write.csv(temp,"xgboost_only_trans.csv",row.names=FALSE)


xgboost@model$variable_importances
```

## rf
```{r}
rf <- h2o.randomForest(x = predictors,
                      y = response,
                      training_frame = train_Hex,
                      validation_frame = val_Hex,
                      ntree=200,
                      seed=950902)

rf

pred_rf_train2 <- h2o.predict(rf,train2_Hex)
pred_rf_val <- h2o.predict(rf,val_Hex)
pred_rf_test <- h2o.predict(rf,test_Hex)

train2$rf <- as.vector(pred_rf_train2$p1)
val$rf <- as.vector(pred_rf_val$p1)
test_df$rf <- as.vector(pred_rf_test$p1)

# temp <- test_df %>% select(msno,rf)
# colnames(temp) <- c("msno","is_churn")
# write.csv(temp,"rf_only_trans.csv",row.names=FALSE)

rf@model$variable_importances
```

## glm
```{r}
glm_x <- h2o.glm(x = predictors,
                  y = response,
                  training_frame = train_Hex,
                      validation_frame = val_Hex,
                 family="binomial",
                      seed=950902)

glm_x

pred_glm_x_train2 <- h2o.predict(glm_x,train2_Hex)
pred_glm_x_val <- h2o.predict(glm_x,val_Hex)
pred_glm_x_test <- h2o.predict(glm_x,test_Hex)

train2$glm <- as.vector(pred_glm_x_train2$p1)
val$glm <- as.vector(pred_glm_x_val$p1)
test_df$glm <- as.vector(pred_glm_x_test$p1)


# temp <- test_df %>% select(msno,glm)
# colnames(temp) <- c("msno","is_churn")
# write.csv(temp,"glm_only_trans.csv",row.names=FALSE)

```

## deeplearning
```{r}
deeplearning <-  h2o.deeplearning(
    training_frame = train_Hex, 
    validation_frame = val_Hex,
    seed = 950902,# validation dataset: used for scoring and early stopping
   x = predictors,
    y = response
   # ,  
#     activation = "Rectifier", # default (a.k.a Relu)
#     hidden = c(200, 200),    # default = 2 hidden layers with 200 neurons each
#     epochs = 1,
#    l1=1.0E-4,
#    input_dropout_ratio= 0.05,
# # How many times the dataset should be iterated
#     variable_importances = TRUE,
#     stopping_metric = "misclassification",
#     stopping_tolerance = 1e-2, # stop when logloss does not improve by >=1% for 2 scoring events
#     stopping_rounds = 2,
#     score_validation_samples = 10000# allows obtaining the variable importance, not enabled by default
)

deeplearning

pred_deeplearning_train2 <- h2o.predict(deeplearning,train2_Hex)
pred_deeplearning_val <- h2o.predict(deeplearning,val_Hex)
pred_deeplearning_test <- h2o.predict(deeplearning,test_Hex)

train2$deeplearning <- as.vector(pred_deeplearning_train2$p1)
val$deeplearning <- as.vector(pred_deeplearning_val$p1)
test_df$deeplearning <- as.vector(pred_deeplearning_test$p1)

# temp <- test_df %>% select(msno,deeplearning)
# colnames(temp) <- c("msno","is_churn")
# write.csv(temp,"deeplearning_only_trans.csv",row.names=FALSE)

deeplearning@model$variable_importances
```

```{r}
train2 <- train2 %>% select(msno, is_churn, glm, rf, gbm, xgboost, deeplearning)
val <- val %>% select(msno, is_churn, glm, rf, gbm, xgboost, deeplearning)
test <- val %>% select(msno, is_churn, glm, rf, gbm, xgboost, deeplearning)
```


## Modeling
```{r}
predictors <- setdiff(names(train2),
                       c("is_churn", "msno"))
response <- "is_churn"
```

## H2O 클러스터
```{r}
train2_Hex<-as.h2o(train2)
val_Hex<-as.h2o(val)
test_Hex<-as.h2o(test)
```

## gbm
```{r}
xgboost_total <- h2o.xgboost(x = predictors,
                      y = response,
                     # family="binomial",
                      training_frame = train2_Hex,
                      validation_frame = val_Hex,
                      ntree=200,
                      seed=950902)

xgboost_total

pred_xgboost_total <- h2o.predict(xgboost_total,val_Hex)
pred_xgboost_test <- h2o.predict(xgboost_total,test_Hex)

val$pred <- as.vector(pred_xgboost_total$C3)
test_df$pred <- as.vector(pred_xgboost_test$C3)

temp <- test_df %>% select(msno, glm, gbm, xgboost, rf, deeplearning)
test_df <- test_df %>% mutate(is_churn = (glm+ gbm+ xgboost+ rf+ deeplearning)/5)

temp <- test_df %>% select(msno,is_churn)
colnames(temp) <- c("msno","is_churn")
write.csv(temp,"mean_regression.csv",row.names=FALSE)

xgboost_total@model$variable_importances
```
